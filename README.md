# ğŸ” AI Research Agent with LangGraph

Um agente inteligente de pesquisa que utiliza LangGraph para orquestrar mÃºltiplos LLMs e APIs de busca, fornecendo respostas abrangentes e bem fundamentadas para perguntas de pesquisa.

## âœ¨ CaracterÃ­sticas

- **GeraÃ§Ã£o Inteligente de Consultas**: Cria automaticamente 3-5 consultas otimizadas para pesquisa
- **Busca Paralela**: Executa mÃºltiplas pesquisas simultaneamente usando Tavily API
- **SÃ­ntese AutomÃ¡tica**: Analisa e resume conteÃºdo web relevante
- **Resposta Estruturada**: Gera respostas de 500-800 palavras com referÃªncias citadas
- **Interface Web**: Interface Streamlit intuitiva e responsiva
- **Modelos Locais**: Utiliza Ollama para execuÃ§Ã£o local dos LLMs

## ğŸ—ï¸ Arquitetura

O projeto utiliza LangGraph para criar um pipeline de processamento com os seguintes nÃ³s:

```
START â†’ build_first_queries â†’ spawn_researches â†’ single_search â†’ final_writer â†’ END
```

### Componentes Principais

- **[`ReportState`](src/schemas.py)**: Estado compartilhado entre os nÃ³s do grafo
- **[`build_first_queries`](src/graph.py)**: Gera consultas de pesquisa baseadas no input do usuÃ¡rio
- **[`single_search`](src/graph.py)**: Executa busca individual e extrai conteÃºdo relevante
- **[`final_writer`](src/graph.py)**: Sintetiza resultados em uma resposta final estruturada

## ğŸ› ï¸ Tecnologias

- **[LangGraph](https://github.com/langchain-ai/langgraph)**: OrquestraÃ§Ã£o de workflows de IA
- **[Ollama](https://ollama.ai/)**: ExecuÃ§Ã£o local de modelos de linguagem
- **[Tavily API](https://tavily.com/)**: API de busca web especializada
- **[Streamlit](https://streamlit.io/)**: Interface web interativa
- **[Pydantic](https://pydantic.dev/)**: ValidaÃ§Ã£o e serializaÃ§Ã£o de dados

## ğŸ“¦ InstalaÃ§Ã£o

### PrÃ©-requisitos

1. **Python 3.13+**
2. **Poetry** (gerenciador de dependÃªncias)
3. **Ollama** instalado localmente
4. **Conta Tavily** para API key

### Passo a Passo

1. **Clone o repositÃ³rio**:
```bash
git clone <repository-url>
cd search-ai-agent-langgraph
```

2. **Instale as dependÃªncias**:
```bash
poetry install
```

3. **Configure o Ollama**:
```bash
# Baixe os modelos necessÃ¡rios
ollama pull gemma2:2b
ollama pull gemma3:1b
```

4. **Configure as variÃ¡veis de ambiente**:
Crie um arquivo `.env` na raiz do projeto:
```env
TAVILY_API_KEY=sua_api_key_aqui
```

5. **Execute a aplicaÃ§Ã£o**:
```bash
poetry run streamlit run app.py
```

## ğŸš€ Uso

### Interface Web

1. Acesse `http://localhost:8501` apÃ³s executar a aplicaÃ§Ã£o
2. Digite sua pergunta de pesquisa na Ã¡rea de texto
3. Clique em "ğŸ” Pesquisar" e aguarde o processamento
4. Visualize a resposta estruturada com referÃªncias

### Uso ProgramÃ¡tico

```python
from src.graph import run_research

# Execute uma pesquisa
resultado = run_research("Quais sÃ£o as tendÃªncias de IA para 2024?")
print(resultado)
```

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ app.py              # Interface Streamlit
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ graph.py        # LÃ³gica principal do grafo LangGraph
â”‚   â”œâ”€â”€ prompts.py      # Templates de prompts para os LLMs
â”‚   â””â”€â”€ schemas.py      # Modelos Pydantic para validaÃ§Ã£o
â”œâ”€â”€ pyproject.toml      # ConfiguraÃ§Ã£o de dependÃªncias
â”œâ”€â”€ poetry.lock         # Lock file das dependÃªncias
â””â”€â”€ README.md           # Este arquivo
```

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### Modelos Ollama

Por padrÃ£o, o projeto usa:
- `gemma2:2b` para raciocÃ­nio complexo
- `gemma3:1b` para tarefas mais simples

Para alterar os modelos, edite [`src/graph.py`](src/graph.py):
```python
llm = ChatOllama(model="seu_modelo_aqui")
reasoning_llm = ChatOllama(model="seu_modelo_de_raciocinio")
```

### PersonalizaÃ§Ã£o de Prompts

Os prompts estÃ£o organizados em [`src/prompts.py`](src/prompts.py):
- `build_queries`: GeraÃ§Ã£o de consultas
- `resume_search`: SÃ­ntese de resultados
- `build_final_response`: Resposta final

## ğŸ“Š Fluxo de Dados

1. **Input do UsuÃ¡rio** â†’ Pergunta inicial
2. **GeraÃ§Ã£o de Consultas** â†’ 3-5 consultas otimizadas
3. **Busca Paralela** â†’ ExecuÃ§Ã£o simultÃ¢nea via Tavily
4. **ExtraÃ§Ã£o de ConteÃºdo** â†’ AnÃ¡lise e sÃ­ntese por LLM
5. **Resposta Final** â†’ Documento estruturado com referÃªncias

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. FaÃ§a fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. Commit suas mudanÃ§as (`git commit -am 'Adiciona nova feature'`)
4. Push para a branch (`git push origin feature/nova-feature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT. Veja o arquivo LICENSE para detalhes.

## ğŸ‘¨â€ğŸ’» Autor

**Jfdeev**
- Email: jfrsoares161205@gmail.com

## ğŸ› Problemas Conhecidos

- Certifique-se de que os modelos Ollama estÃ£o baixados e funcionando
- A API key do Tavily deve estar configurada corretamente
- Algumas consultas podem demorar devido ao processamento sequencial de extraÃ§Ã£o

## ğŸ”® Roadmap

- [ ] Suporte a mÃºltiplos idiomas de resposta
- [ ] Cache de resultados de pesquisa
- [ ] Interface para seleÃ§Ã£o de modelos
- [ ] ExportaÃ§Ã£o de relatÃ³rios em PDF
- [ ] IntegraÃ§Ã£o com outras APIs de busca